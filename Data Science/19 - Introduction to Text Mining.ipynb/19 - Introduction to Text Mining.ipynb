{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we look at a range of text mining techniques, available as part of Scikit-learn.\n",
    "\n",
    "As our sample corpus of text, we will read a collection of news articles. These articles have been stored in a single file and formatted so that one article appears on each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(\"news-articles.txt\",\"r\")\n",
    "raw_documents = fin.readlines()\n",
    "fin.close()\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw text documents are textual, not numeric. The first step in analysing unstructured documents is to split the raw text into individual tokens, each corresponding to a single term (word). As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = raw_documents[0]\n",
    "# print a snippet\n",
    "print(doc1[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built-in scikit-learn tokenizer to split this document into tokens. Note that we will perform *case conversion* first to convert the entire text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokenize = CountVectorizer().build_tokenizer()\n",
    "# convert to lowercase, then tokenize\n",
    "tokens1 = tokenize(doc1.lower())\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately see that many of the words here are not useful (e.g. \"to\", \"the\" etc.). Scikit-learn provides a list of such *stop words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter out these stopwords from our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens1 = []\n",
    "for token in tokens1:\n",
    "    if not token in stopwords:\n",
    "        filtered_tokens1.append(token)\n",
    "print(filtered_tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat this process for all documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filtered_tokens = []\n",
    "for doc in raw_documents:\n",
    "    # tokenize the next document\n",
    "    tokens = tokenize(doc.lower())\n",
    "    # remove the stopwords\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if not token in stopwords:\n",
    "            filtered_tokens.append(token)  \n",
    "    # add to the overall list\n",
    "    all_filtered_tokens.append( filtered_tokens )\n",
    "print(\"Created %d filtered token lists\" % len(all_filtered_tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple type of analysis that we might do is to count the number of times specific terms (words) appear in our corpus. We could do this by creating a dictionary of term frequency counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "# process filtered tokens for each document\n",
    "for doc_tokens in all_filtered_tokens:\n",
    "    for token in doc_tokens:\n",
    "        # increment existing?\n",
    "        if token in counts:\n",
    "            counts[token] += 1\n",
    "        # a new term?\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "print(\"Found %d unique terms in this corpus\" % len(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find the terms in the dictionary with the highest counts. Python provides a convenient way of doing this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above creates a list of tuple pairs, where the first value is the key (i.e. the term) and the second value is the value (i.e the count). Let's display the top 20 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    term = sorted_counts[i][0]\n",
    "    count = sorted_counts[i][1]\n",
    "    print( \"%s (count=%d)\" % ( term, count )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bag-of-Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the *bag-of-words model*, each document is represented by a vector in a *m*-dimensional coordinate space, where *m* is number of unique terms across all documents. This set of terms is called the corpus *vocabulary*. Note that the positioning (context) of terms within the original document is lost in this model.\n",
    "\n",
    "Since each document can be represented as a term vector, we can stack these vectors to create a full *document-term matrix*. We can easily create this matrix from a list of document strings using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process also build a vocabulary for the corpus, both in the form of a list and in the form of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some sample terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terms[500:530])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each column in the document-term matrix correspond to a term, we can look up the column associated with each term using the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what column is the term 'football' on?\n",
    "vocab[\"football\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what column is the term 'world' on?\n",
    "vocab[\"world\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same Scikit-learn functionality to create a document-term matrix with N-grams. We specify an extra parameter ngram_range which specifies the shortest and longest token sequences to include. Length 1 is just a single token.\n",
    "\n",
    "For instance, transform our input documents into a matrix, extracting single tokens and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2))\n",
    "X = vectorizer.fit_transform(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the vocabulary is much larger now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some sample terms. Note that we see a mix of single tokens and bigrams (i.e. phrases of length 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terms[510:520])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A range of steps can be used to process text input files to reduce the number of terms used to represent the text and to improve the resulting bag-of-words model. These include:\n",
    "- Minimum term length: Exclude terms of length < 2. Scikit-learn does this by default.\n",
    "- Case conversion: Converting all terms to lowercase. Scikit-learn does this by default.\n",
    "- Stop-word filtering: Remove terms that appear on a pre-defined \"blacklist\" of terms that are highly frequent and do- not convey useful information.\n",
    "- Low frequency filtering: Remove terms that appear in very few documents.\n",
    "- Stemming: Reduce words to their stems (or base forms).\n",
    "\n",
    "Scikit-learn allows us to perform one or more of these steps by adapting the CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in list of stop-words for a given language by just specifying the name of the language (lower-case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# Are standard stopwords gone?\n",
    "\"and\" in vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use our own custom stop-word list, which might be more appropriate for specific applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [ \"and\", \"the\", \"game\" ] \n",
    "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# Are custom stopwords gone?\n",
    "\"game\" in vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove low frequency terms that appear in fewer than a specified number of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many terms did we have with the last approach?\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(\"Number of terms in model is %d\" % len(vectorizer.vocabulary_) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build another matrix, but filter terms appearing in less than 5 documents\n",
    "vectorizer = CountVectorizer(min_df = 5)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(\"Number of terms in model is %d\" % len(vectorizer.vocabulary_) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stem tokens to their base form, we need to use functionality from another third party library: **NLTK**. You may need to install this package manually, either through the Conda interface or via the Conda command line tool, using:\n",
    "    \n",
    "    conda install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out the standard English stemming algorithm (called the Porter Stemmer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the standard English stemming algorithm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "words = ['plotted', 'flies', 'denied', 'sales', 'seas', 'computing', 'computed']\n",
    "# try stemming each sample word\n",
    "stemmer = PorterStemmer()\n",
    "for w in words:\n",
    "    print( stemmer.stem(w) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use NLTK stemming with Scikit-learn, we need to create a custom tokenisation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# define the function\n",
    "def stem_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform stemming on each token\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append( stemmer.stem(token) )\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our custom tokenizer with the standard CountVectorizer approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=stem_tokenizer)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# display some sample terms\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(terms[200:220])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform lemmatisation in the same way, using NLTK with Sckit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def lemma_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append( lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can use our custom tokenizer with the standard CountVectorizer approach. The output terms are somewhat easier to intrepret than those produced by stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# display some sample terms\n",
    "print(list(vectorizer.vocabulary_.keys())[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all of these steps together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\",min_df = 3,tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(vectorizer.vocabulary_.keys())[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as including/excluding terms, we can also modify or weight the frequency values themselves. We can improve the usefulness of the document-term matrix by giving more weight to the more \"important\" terms.\n",
    "\n",
    "The most common normalisation is *term frequency–inverse document frequency* (TF-IDF). In Scikit-learn, we can generate at TF-IDF weighted document-term matrix by using TfidfVectorizer() in place of CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",min_df = 5)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# display some sample weighted values\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cosine similarity*: Most common approach for measuring similarity between two documents in a bag-of-words representation is to look at the cosine of the angle between their corresponding two term vectors. The motivation is that vectors for documents containing similar terms will point in the same direction in the m-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's find the most similar document to the first document in our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First document - just display the start of it\n",
    "print(raw_documents[0][0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Measure the cosine similarity between the first document vector and all of the others\n",
    "max_cos = 0\n",
    "best_row = 0\n",
    "for row in range(1,X.shape[0]):\n",
    "    cos = cosine_similarity( X[0], X[row] )\n",
    "    # best so far?\n",
    "    if cos > max_cos:\n",
    "        max_cos = cos\n",
    "        best_row = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most similar document was row %d: cosine similarity = %.3f\" % ( best_row, max_cos ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best document - just display the start of it\n",
    "print(raw_documents[best_row][0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
