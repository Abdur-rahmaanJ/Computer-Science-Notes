# Reviewing Policy Gradient methods

>Continuous states and actions in high dimensional spaces cannot be treated by most off-the-shelf reinforcement learning approaches. Policy gradient methods differ significantly as they do not suffer from these problems in the same way. For example, uncertainty in the state might degrade the performance of the policy (if no additional state estimator is being used) but the optimization techniques for the policy do not need to be changed. Continuous states and actions can be dealt with in exactly the same way as discrete ones while, in addition, the learning performance is often increased. Convergence at least to a local optimum is guaranteed (**critical for robotics**).

>The advantages of policy gradient methods for real world applications are numerous. Among the most important ones are that the policy representations can be chosen so that it is meaningful for the task and can incorporate domain knowledge, that often fewer parameters are needed in the learning process than in value-function based approaches and that there is a variety of different algorithms for policy gradient estimation in the literature which have a rather strong theoretical underpinning. Additionally, policy gradient methods can be used either model-free or model-based as they are a generic formulation.

Policy gradient methods however,
> are by definition on-policy (note that tricks like importance sampling can slightly alleviate this problem) and need to forget data very fast in order to avoid the introduction of a bias to the gradient estimator. Hence, the use of sampled data is not very efficient. In tabular representations, value function methods are guaranteed to converge to a global maximum while policy gradients only converge to a local maximum and there may be many maxima in discrete problems. Policy gradient methods are often quite demanding to apply, mainly because one has to have considerable knowledge about the system one wants to control to make reasonable policy definitions. Finally, policy gradient methods always have an open parameter, the learning rate, which may decide over the order of magnitude of the speed of convergence, these have led to new approaches inspired by expectation-maximization (see, e.g., Vlassis et al., 2009; Kober & Peters, 2008).


### Notation:
The three main components of a Reinfocement Learning (RL) system for robotics include the state <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode" align=middle width=7.705549500000004pt height=14.155350000000013pt/> (also found in literature as <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>), the action <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/44bc9d542a92714cac84e01cbbb7fd61.svg?invert_in_darkmode" align=middle width=8.689230000000004pt height=14.155350000000013pt/> (also found as <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/6dbb78540bd76da3f1625782d42d6d16.svg?invert_in_darkmode" align=middle width=9.410280000000004pt height=14.155350000000013pt/>) and the reward denoted by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode" align=middle width=7.873024500000003pt height=14.155350000000013pt/>. We will denote the current time step by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/>. The stochasticity of the environment gets represented by using a probability distribution <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/4b43b99577e49c30237a75b10434a256.svg?invert_in_darkmode" align=middle width=159.496755pt height=24.65759999999998pt/> as model where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/687ec27e46c5b07ba8adaab70976974e.svg?invert_in_darkmode" align=middle width=63.0102pt height=27.656969999999987pt/> denotes the current action and <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/59efeb0f4f5d484a9b8a404d5bdac544.svg?invert_in_darkmode" align=middle width=14.971605000000004pt height=14.155350000000013pt/>, <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/eaa3f599744549c4805d08b50d0b9d3d.svg?invert_in_darkmode" align=middle width=75.79803pt height=27.656969999999987pt/> denote the current and next state, respectively. Further, we assume that actions are generated by a policy <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/e2f0dada90b45ef6dd58f15372396f59.svg?invert_in_darkmode" align=middle width=113.628075pt height=24.65759999999998pt/> which is modeled as a probability distribution in order to incorporate exploratory actions. The policy is assumed to be parameterized by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/d6328eaebbcd5c358f426dbea4bdbf70.svg?invert_in_darkmode" align=middle width=15.137100000000004pt height=22.46574pt/> policy parameters <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/2b37baa87099988c8cd4a6844baa66ff.svg?invert_in_darkmode" align=middle width=61.029705pt height=27.656969999999987pt/>. The sequence of states and actions forms a trajectory denoted by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/85e4a233f33b0a54f2020db13ef59cc6.svg?invert_in_darkmode" align=middle width=109.53294pt height=24.65759999999998pt/> where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/7b9a0316a2fcd7f01cfd556eedf72e96.svg?invert_in_darkmode" align=middle width=14.999985000000004pt height=22.46574pt/> denoted the horizon which can be infinite. Often, *trajectory*, *history*, *trial* or *roll-out* are used interchangeably. At each instant of time, the learning system receives a reward <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/d18b0f40de3e43b4fd5efdfec3abebff.svg?invert_in_darkmode" align=middle width=141.130275pt height=24.65759999999998pt/>.

The general goal of policy optimization in reinforcement learning for robots is to optimize the policy parameters <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/1995b01bb3050d9d9821e07ddcbe14e8.svg?invert_in_darkmode" align=middle width=51.988530000000004pt height=27.656969999999987pt/> so that the expected return is optimized:
<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/4bccbc51eab4b628974e59227b4692ea.svg?invert_in_darkmode" align=middle width=184.80825pt height=39.45249pt/></p>

 where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/11c596de17c342edeed29f489aa4b274.svg?invert_in_darkmode" align=middle width=9.423975000000004pt height=14.155350000000013pt/> is <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/30fe7dcc0f0b09e88904b9ec5db07243.svg?invert_in_darkmode" align=middle width=64.21833pt height=24.65759999999998pt/> for discounted reinforcement learning and  <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/15aeb611e303b3dcd763a3b0a1217f02.svg?invert_in_darkmode" align=middle width=44.960355pt height=27.775769999999994pt/> for the average reward case.

 For real-world applications, we require that any change to the policy parameterization has to be smooth as drastic changes can be hazardous for the actor as well as useful initializations of the policy based on domain knowledge would otherwise vanish after a single update step. For these reasons, policy gradient methods which follow the steepest descent on the expected return are the method of choice. These methods update the policy parameterization according to the gradient update rule
<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/01d468f612d0181612fabfbd9d4a0b0b.svg?invert_in_darkmode" align=middle width=189.98595pt height=19.004534999999997pt/></p>

where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/57be6b17a68c3953d4c5ded97094ac3d.svg?invert_in_darkmode" align=middle width=61.088280000000005pt height=26.177579999999978pt/> denotes a learning rate and <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/8a2311b498f76d3e234595dbe724077c.svg?invert_in_darkmode" align=middle width=111.754005pt height=24.65759999999998pt/> the current update number.

| Term | Definition |
|------|------------|
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode" align=middle width=7.705549500000004pt height=14.155350000000013pt/> | the state (also found in literature as <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>) |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/44bc9d542a92714cac84e01cbbb7fd61.svg?invert_in_darkmode" align=middle width=8.689230000000004pt height=14.155350000000013pt/> | the action (also found as <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/6dbb78540bd76da3f1625782d42d6d16.svg?invert_in_darkmode" align=middle width=9.410280000000004pt height=14.155350000000013pt/>) |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode" align=middle width=7.873024500000003pt height=14.155350000000013pt/> | the reward |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/> | time step |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/4b43b99577e49c30237a75b10434a256.svg?invert_in_darkmode" align=middle width=159.496755pt height=24.65759999999998pt/> |  probability distribution representing the stochasticity of the environment |
| ... | ... |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/5b16931dbf74748d0ce023fa18b1fa50.svg?invert_in_darkmode" align=middle width=358.966905pt height=47.67179999999999pt/> |  discount factor |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/0fe1677705e987cac4f589ed600aa6b3.svg?invert_in_darkmode" align=middle width=9.046950000000002pt height=14.155350000000013pt/> | trajectories |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/160a51dd5dc4e05542d6540b3c1f29b7.svg?invert_in_darkmode" align=middle width=143.45627999999996pt height=24.65759999999998pt/> | roll-outs |
| <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/71e2e285c679246c5443054b075b010d.svg?invert_in_darkmode" align=middle width=120.55230000000002pt height=27.656969999999987pt/> | return in the roll-outs |
| ... | ... |

### Vanilla Policy Gradient

The main problem in policy gradient methods is to obtain a good estimator of the policy gradient <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/26cffbf0e87b9aec1aa3d9539f35712c.svg?invert_in_darkmode" align=middle width=80.59755pt height=24.65759999999998pt/> In robotics, people have traditionally used deterministic model-based methods for obtaining the gradient however, this approach does not scale well as one needs to be able to model every detail of the system. Alternatively, we can estimate the policy gradient simply from data generated during the execution of a task, i.e., without the need for a model. The most prominent approaches for policy gradient estimation, which have been applied to robotics are: finite-difference and likelihood ratio methods (better known as REINFORCE in the RL community). According to (cite PG methods paper), of these two, the latter has a variety of advantages in comparison to finite difference methods when applied to robotics.

In likelihood ration methods we assume that trajectories <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/0fe1677705e987cac4f589ed600aa6b3.svg?invert_in_darkmode" align=middle width=9.046950000000002pt height=14.155350000000013pt/> are generated from a system by roll-outs, i.e., <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/160a51dd5dc4e05542d6540b3c1f29b7.svg?invert_in_darkmode" align=middle width=143.45627999999996pt height=24.65759999999998pt/> with return <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/71e2e285c679246c5443054b075b010d.svg?invert_in_darkmode" align=middle width=120.55230000000002pt height=27.656969999999987pt/> which leads to <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/1f13a97e44eb6dfb4b61566c3a5b3ca7.svg?invert_in_darkmode" align=middle width=246.778455pt height=26.48447999999999pt/>. In this case, the policy gradient can be estimated using the likelihood ratio (see e.g. Glynn, 1987; Aleksandrov, Sysoyev, and Shemeneva, 1968) better known as REINFORCE (Williams, 1992) trick, i.e., by using:

<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/2f017123f4cf9dbcee844a9c99308846.svg?invert_in_darkmode" align=middle width=214.4802pt height=16.438356pt/></p>

from standard differential calculus <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/dc912efa02a4c541773a11456558a6a8.svg?invert_in_darkmode" align=middle width=222.70000499999998pt height=24.65759999999998pt/>, we obtain

<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/17a6b400677f04e1fd7dd68690d098ac.svg?invert_in_darkmode" align=middle width=614.8098pt height=37.352039999999995pt/></p>

As the expectation <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/5adb523f872c74e09aef72f6f60f99d2.svg?invert_in_darkmode" align=middle width=34.086855pt height=24.65759999999998pt/> can be replaced by sample averages, denoted by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/c76e2a65d080aee7e7edc3a5c6016798.svg?invert_in_darkmode" align=middle width=17.351730000000003pt height=24.65759999999998pt/> , only the derivative <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/00c68679ad16e5f26b1763f62354214a.svg?invert_in_darkmode" align=middle width=88.12782pt height=24.65759999999998pt/> is needed for the estimator. Importantly, this derivative can be computed without knowledge of the generating distribution <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/d78038727f16aca35dc5f84f47ba0874.svg?invert_in_darkmode" align=middle width=40.2798pt height=24.65759999999998pt/> as:

<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/ee3138b1f1f2fec563f334c82eee4da6.svg?invert_in_darkmode" align=middle width=331.86285pt height=30.922649999999997pt/></p>

which implies that:

<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/b810c07f63630da6c6b8bee6106c0d03.svg?invert_in_darkmode" align=middle width=283.5426pt height=30.922649999999997pt/></p>

as only the policy depends on <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/27e556cf3caa0673ac49a8f0de3c73ca.svg?invert_in_darkmode" align=middle width=8.173588500000005pt height=22.831379999999992pt/>. Thus, the derivatives of <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/857606fb064ea1680d2dc215438d7336.svg?invert_in_darkmode" align=middle width=105.39044999999999pt height=24.65759999999998pt/> do not have to be computed and no model needs to be maintained. However, if we had a deterministic policy <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/40a32049a9ecb5345010ceaf9f93ab11.svg?invert_in_darkmode" align=middle width=61.309214999999995pt height=24.65759999999998pt/> instead of a stochastic policy <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/67129ee39dc2819f1f47ac6d7c7e5461.svg?invert_in_darkmode" align=middle width=75.064935pt height=24.65759999999998pt/> , computing such a derivative would require the derivative <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/1b70b8f3ae4ab07ccc567edb1ac3990e.svg?invert_in_darkmode" align=middle width=407.96035499999994pt height=24.65759999999998pt/> and, hence, it would require a system model.

In order to reduce the variance of the gradient estimator, a constant baseline can be subtracted from the gradient, i.e.,

<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/71baaea0aa6ae52b03a297979e77926d.svg?invert_in_darkmode" align=middle width=270.21225pt height=16.438356pt/></p>

where the baseline <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/278878512269e8ec072ffd7106474c1b.svg?invert_in_darkmode" align=middle width=39.018209999999996pt height=22.831379999999992pt/> can be chosen arbitrarily (Williams, 1992). It is straightforward to show that this baseline does not introduce bias in the gradient as differentiating <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/ab0ae96fa3c555e144140b7d738508a1.svg?invert_in_darkmode" align=middle width=109.75436999999998pt height=26.48447999999999pt/> implies that:

<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/18b8346d537952cea47e65c1bf0f2b12.svg?invert_in_darkmode" align=middle width=142.30557pt height=37.352039999999995pt/></p>

and, hence, the constant baseline will vanish for infinite data while reducing the variance of the gradient estimator for finite data. See Peters & Schaal, 2008 for an overview of how to choose the baseline optimally. Therefore, the general path likelihood ratio estimator or episodic REINFORCE gradient estimator is given by

<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/7a444da55d1ce5fffbe31d3b30422081.svg?invert_in_darkmode" align=middle width=404.02394999999996pt height=39.45249pt/></p>

where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/7759cb428e2d3ec6006c12264042547c.svg?invert_in_darkmode" align=middle width=17.351730000000003pt height=24.65759999999998pt/> denotes the average over trajectories. This type of method is guaranteed to converge to the true gradient at the fastest theoretically possible error decrease of <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/a75d3f76675eb9c964afc4a6bc7c57b9.svg?invert_in_darkmode" align=middle width=70.25584500000001pt height=29.19113999999999pt/> where I denotes the number of roll-outs (Glynn, 1987) even if the data is generated from a highly stochastic system.

Pseudocode:
```
1. Initialize policy (e.g. NNs) parameter <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/27e556cf3caa0673ac49a8f0de3c73ca.svg?invert_in_darkmode" align=middle width=8.173588500000005pt height=22.831379999999992pt/> and baseline <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/>
2. For iteration=1,2,... do
    2.1 Collect a set of trajectories by executing the current policy obtaining $\mathbf{s}_{0:H},\mathbf{a}_{0:H},r_{0:H}$
    2.2 At each timestep in each trajectory, compute
        2.2.1 the return $R_t = \sum_{t'=t}^{T-1} \gamma^{t'-t}r_{t'}$ and
        2.2.2 the advantage estimate $\hat{A_t} = R_t - b(s_t)$.
    2.3 Re-fit the baseline (recomputing the value function) by minimizing
        $|| b(s_t) - R_t||^2$, summed over all trajectories and timesteps.

          $b=\frac{\left\langle \left(  \sum\nolimits_{h=0}^{H} \mathbf{\nabla}_{\theta_{k}}\log\pi_{\mathbf{\theta}}\left(  \mathbf{a}_{h}\left\vert \mathbf{s}_{h}\right.  \right)  \right)  ^{2}\sum\nolimits_{l=0}^{H} \gamma r_{l}\right\rangle }{\left\langle \left(
          \sum\nolimits_{h=0}^{H}\mathbf{\nabla}_{\theta_{k}}\log\pi_{\mathbf{\theta}
          }\left(  \mathbf{a}_{h}\left\vert \mathbf{x}_{h}\right.  \right)  \right)
          ^{2}\right\rangle }$

    2.4 Update the policy, using a policy gradient estimate $\hat{g}$,
        which is a sum of terms $\nabla_\theta log\pi(a_t | s_t,\theta)\hat(A_t)$.
        In other words:

          $g_{k}=\left\langle \left(  \sum\nolimits_{h=0}^{H}\mathbf{\nabla
          }_{\theta_{k}}\log\pi_{\mathbf{\theta}}\left(  \mathbf{a}_{h}\left\vert
          \mathbf{s}_{h}\right.  \right)  \right)  \left(  \sum\nolimits_{l=0}^{H}
          \gamma r_{l}-b\right)  \right\rangle$
3. **end for**
```

### Stochastic vs deterministic policy
From https://www.quora.com/Whats-the-difference-between-deterministic-policy-gradient-and-stochastic-policy-gradient:

>In stochastic policy gradient, actions are drawn from a distribution parameterized by your policy. For example, your robot’s motor torque might be drawn from a Normal distribution with mean μμ and deviation σσ. Where your policy will predict μμ and σσ. When you draw from this distribution and evaluate your policy, you can move your mean closer to samples that led to higher reward and farther from samples that led to lower reward, and reduce your deviation as you become more confident.
>
>When you reduce the variance to 0, we get a policy that is deterministic. In deterministic policy gradient, we directly take the gradients of μμ.
>
>In the stochastic case, the policy gradient integrates over both state and action spaces, whereas in the deterministic case it only integrates over the state space. As a result, computing the stochastic policy gradient may require more samples, especially if the action space has many dimensions.

### Code

A discrete implementation of VPG can be found [here](code/train_cartpole_pg.py). The code includes comments with the pseudo-code presented above for readibility.



# Sources:
- http://www.scholarpedia.org/article/Policy_gradient_methods#Likelihood_Ratio_Methods_and_REINFORCE
- https://theneuralperspective.com/2016/10/27/gradient-topics/
- https://theneuralperspective.com/2016/11/25/reinforcement-learning-rl-policy-gradients-i/
- https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient
